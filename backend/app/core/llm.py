import google.generativeai as genai
from app.core.config import settings
from typing import List, Dict

class LLMClient:
    def __init__(self):
        self.api_key = settings.GEMINI_API_KEY
        print(f"DEBUG: LLMClient initializing. Key loaded: '{self.api_key[:10]}...' (len={len(self.api_key)})")
        if self.api_key and "YOUR_" not in self.api_key:
            try:
                genai.configure(api_key=self.api_key)
                # gemini-1.5-flash failed, using gemini-flash-latest from list
                self.model = genai.GenerativeModel('gemini-flash-latest')
                self.client_ready = True
                print("DEBUG: LLMClient Ready (Real Gemini)")
            except Exception as e:
                print(f"DEBUG: Failed to configure GenAI: {e}")
                self.client_ready = False
        else:
            self.client_ready = False
            print("WARNING: Gemini API Key not set. Using Mock LLM.")

    def generate(self, prompt: str) -> str:
        """
        Generates text based on prompt using Gemini.
        """
        if not self.client_ready:
            return self._mock_response(prompt)
        
        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            error_str = str(e).lower()
            if "429" in error_str or "quota" in error_str or "resource" in error_str:
                print(f"CRITICAL: AI Quota Exceeded. {e}")
                raise Exception("AI_QUOTA_EXCEEDED")
                
            print(f"LLM Error: {e}")
            return self._mock_response(prompt)

    def chat_completion(self, messages: List[Dict[str, str]]) -> str:
        """
        Handles chat completion by converting messages to a single prompt string.
        Adaptor for models that prefer single prompt or for simplicity.
        """
        if not self.client_ready:
            # Just use the last message for mock
            last_msg = messages[-1].get("content", "")
            return self._mock_response(last_msg)

        # Convert history validation to a prompt string
        # Gemini does support history objects, but string concatenation is robust for basic usage
        full_prompt = ""
        for msg in messages:
            role = msg.get("role", "user").upper()
            content = msg.get("content", "")
            full_prompt += f"\n{role}: {content}"
        
        full_prompt += "\nASSISTANT: "
        
        return self.generate(full_prompt)

    def _mock_response(self, prompt: str) -> str:
        """
        Fallback mock responses.
        """
        prompt_lower = prompt.lower()
        
        if "syllabus" in prompt_lower or "priority" in prompt_lower or "quiz" in prompt_lower:
            # Return a JSON list of strings as expected by SkillIdentifierAgent
            return '["Variables", "Data Types", "Control Structures", "Functions", "OOP", "Data Structures"]'
            
        elif "psychometric" in prompt_lower:
            return '{"learning_preference": "Practical", "learning_speed": "Moderate", "difficulty_comfort": "Medium", "feedback_style": "Hints", "goal_orientation": "Projects"}'
            
        elif "explain" in prompt_lower or "concept" in prompt_lower:
             return "This is a concept explanation generated by the mock LLM agent (GenAI Key Missing). Python variables are containers for storing data values."
             
        elif "code" in prompt_lower:
            return "def example():\n    print('Hello World from Mock Agent')"
            
        return "I am a mock AI agent (Gemini Key missing). processed: " + prompt[:50] + "..."

llm_client = LLMClient()
